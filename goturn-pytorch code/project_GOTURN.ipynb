{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "#from torchvision import models\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "# necessary imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import model\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "# from torchsummary import summary\n",
    "\n",
    "# from datasets import ALOVDataset, ILSVRC2014_DET_Dataset\n",
    "from helper import (Rescale, shift_crop_training_sample,crop_sample, NormalizeToTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if cuda else 'cpu')\n",
    "input_size = 224\n",
    "kSaveModel = 20000  # save model after every 20000 steps\n",
    "batchSize = 50  # number of samples in a batch\n",
    "kGeneratedExamplesPerImage = 10  # generate 10 synthetic samples per image\n",
    "transform = NormalizeToTensor()\n",
    "bb_params = {}\n",
    "enable_tensorboard = False\n",
    "if enable_tensorboard:\n",
    "    from tensorboardX import SummaryWriter\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "#parser = argparse.ArgumentParser(description='GOTURN Training')\n",
    "global lr, lr_decay_step, gamma,num_batches, momentum, weight_decay,save_directory,lshift,lscale,minsc,maxsc,seed,save_freq,resume, b\n",
    "num_batches=500000\n",
    "lr=1e-5\n",
    "gamma=0.1\n",
    "momentum=0.9\n",
    "weight_decay=0.0005\n",
    "lr_decay_step=100000\n",
    "save_directory='../saved_checkpoints/exp3/'\n",
    "lshift=5\n",
    "lscale=15\n",
    "minsc=-0.4\n",
    "maxsc=0.4\n",
    "seed=800\n",
    "resume=''\n",
    "b=50\n",
    "save_freq=20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GoNet(nn.Module):\n",
    "    \"\"\" Neural Network class\n",
    "        Two stream model:\n",
    "        ________\n",
    "       |        | conv layers              Untrained Fully\n",
    "       |Previous|------------------>|      Connected Layers\n",
    "       | frame  |                   |    ___     ___     ___\n",
    "       |________|                   |   |   |   |   |   |   |   fc4\n",
    "                   Pretrained       |   |   |   |   |   |   |    * (left)\n",
    "                   CaffeNet         |-->|fc1|-->|fc2|-->|fc3|--> * (top)\n",
    "                   Convolution      |   |   |   |   |   |   |    * (right)\n",
    "                   layers           |   |___|   |___|   |___|    * (bottom)\n",
    "        ________                    |   (4096)  (4096)  (4096)  (4)\n",
    "       |        |                   |\n",
    "       | Current|------------------>|\n",
    "       | frame  |\n",
    "       |________|\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(GoNet, self).__init__()\n",
    "        caffenet = models.alexnet(pretrained=True)\n",
    "        self.convnet = nn.Sequential(*list(caffenet.children())[:-1])\n",
    "        for param in self.convnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.classifier = nn.Sequential(\n",
    "                nn.Linear(256*6*6*2, 4096),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(4096, 4),\n",
    "                )\n",
    "        self.weight_init()\n",
    "\n",
    "    def weight_init(self):\n",
    "        for m in self.classifier.modules():\n",
    "            # fully connected layers are weight initialized with\n",
    "            # mean=0 and std=0.005 (in tracker.prototxt) and\n",
    "            # biases are set to 1\n",
    "            # tracker.prototxt link: https://goo.gl/iHGKT5\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.bias.data.fill_(1)\n",
    "                m.weight.data.normal_(0, 0.005)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x1 = self.convnet(x)\n",
    "        x1 = x1.view(x.size(0), 256*6*6)\n",
    "        x2 = self.convnet(y)\n",
    "        x2 = x2.view(x.size(0), 256*6*6)\n",
    "        x = torch.cat((x1, x2), 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_batch(num_running_batch, running_batch, dataset):\n",
    "    '''\n",
    "    Implements GOTURN batch formation regimen.\n",
    "    '''\n",
    "    global batchSize\n",
    "    done = False\n",
    "    N = kGeneratedExamplesPerImage+1\n",
    "    train_batch = None\n",
    "    x1_batch, x2_batch, y_batch = make_transformed_samples(dataset)\n",
    "    assert(x1_batch.shape[0] == x2_batch.shape[0] == y_batch.shape[0] == N)\n",
    "    count_in = min(batchSize - num_running_batch, N)\n",
    "    remain = N - count_in\n",
    "    running_batch['previmg'][num_running_batch:\n",
    "                             num_running_batch+count_in] = x1_batch[:count_in]\n",
    "    running_batch['currimg'][num_running_batch:\n",
    "                             num_running_batch+count_in] = x2_batch[:count_in]\n",
    "    running_batch['currbb'][num_running_batch:\n",
    "                            num_running_batch+count_in] = y_batch[:count_in]\n",
    "    num_running_batch = num_running_batch + count_in\n",
    "    if remain > 0:\n",
    "        done = True\n",
    "        train_batch = running_batch.copy()\n",
    "        running_batch['previmg'][:remain] = x1_batch[-remain:]\n",
    "        running_batch['currimg'][:remain] = x2_batch[-remain:]\n",
    "        running_batch['currbb'][:remain] = y_batch[-remain:]\n",
    "        num_running_batch = remain\n",
    "    return running_batch, train_batch, done, num_running_batch\n",
    "\n",
    "\n",
    "def make_transformed_samples(dataset):\n",
    "    '''\n",
    "    Given a dataset, it picks a random sample from it and returns a batch\n",
    "    of (kGeneratedExamplesPerImage+1) samples. The batch contains true sample\n",
    "    from dataset and kGeneratedExamplesPerImage samples, which are created\n",
    "    artifically with augmentation by GOTURN smooth motion model.\n",
    "    '''\n",
    "    idx = np.random.randint(len(dataset), size=1)[0]\n",
    "    # unscaled original sample (single image and bb)\n",
    "    orig_sample = dataset.get_orig_sample(idx)\n",
    "    # cropped scaled sample (two frames and bb)\n",
    "    true_sample, _ = dataset.get_sample(idx)\n",
    "    true_tensor = transform(true_sample)\n",
    "    x1_batch = torch.Tensor(kGeneratedExamplesPerImage + 1, 3,\n",
    "                            input_size, input_size)\n",
    "    x2_batch = torch.Tensor(kGeneratedExamplesPerImage + 1, 3,\n",
    "                            input_size, input_size)\n",
    "    y_batch = torch.Tensor(kGeneratedExamplesPerImage + 1, 4)\n",
    "\n",
    "    # initialize batch with the true sample\n",
    "    x1_batch[0] = true_tensor['previmg']\n",
    "    x2_batch[0] = true_tensor['currimg']\n",
    "    y_batch[0] = true_tensor['currbb']\n",
    "\n",
    "    scale = Rescale((input_size, input_size))\n",
    "    for i in range(kGeneratedExamplesPerImage):\n",
    "        sample = orig_sample\n",
    "        # unscaled current image crop with box\n",
    "        curr_sample, opts_curr = shift_crop_training_sample(sample, bb_params)\n",
    "        # unscaled previous image crop with box\n",
    "        prev_sample, opts_prev = crop_sample(sample)\n",
    "        scaled_curr_obj = scale(curr_sample, opts_curr)\n",
    "        scaled_prev_obj = scale(prev_sample, opts_prev)\n",
    "        training_sample = {'previmg': scaled_prev_obj['image'],\n",
    "                           'currimg': scaled_curr_obj['image'],\n",
    "                           'currbb': scaled_curr_obj['bb']}\n",
    "        sample = transform(training_sample)\n",
    "        x1_batch[i+1] = sample['previmg']\n",
    "        x2_batch[i+1] = sample['currimg']\n",
    "        y_batch[i+1] = sample['currbb']\n",
    "\n",
    "    return x1_batch, x2_batch, y_batch\n",
    "\n",
    "\n",
    "def train_model(model, datasets, criterion, optimizer):\n",
    "\n",
    "    global writer\n",
    "    since = time.time()\n",
    "    curr_loss = 0\n",
    "    flag = False\n",
    "    start_itr = 0\n",
    "    num_running_batch = 0\n",
    "    running_batch = {'previmg': torch.Tensor(batchSize, 3, input_size, input_size),\n",
    "                     'currimg': torch.Tensor(batchSize, 3, input_size, input_size),\n",
    "                     'currbb': torch.Tensor(batchSize, 4)}\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "                                          step_size=lr_decay_step,\n",
    "                                          gamma=gamma)\n",
    "    \n",
    "    # resume from a checkpoint\n",
    "    if resume:\n",
    "        if os.path.isfile(resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(resume))\n",
    "            checkpoint = torch.load(resume)\n",
    "            start_itr = checkpoint['itr']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "            num_running_batch = checkpoint['num_running_batch']\n",
    "            running_batch = checkpoint['running_batch']\n",
    "            lr = checkpoint['lr']\n",
    "            np.random.set_state(checkpoint['np_rand_state'])\n",
    "            torch.set_rng_state(checkpoint['torch_rand_state'])\n",
    "            print(\"=> loaded checkpoint '{}' (iteration {})\"\n",
    "                  .format(resume, checkpoint['itr']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(resume))\n",
    "\n",
    "    if not os.path.isdir(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    itr = start_itr\n",
    "    st = time.time()\n",
    "    while itr < num_batches:\n",
    "\n",
    "        model.train()\n",
    "        if (resume and os.path.isfile(resume) and\n",
    "           itr == start_itr and (not flag)):\n",
    "            checkpoint = torch.load(resume)\n",
    "            i = checkpoint['dataset_indx']\n",
    "            flag = True\n",
    "        else:\n",
    "            i = 0\n",
    "\n",
    "        # train on datasets\n",
    "        # usually ALOV and ImageNet\n",
    "        while i < len(datasets):\n",
    "            dataset = datasets[i]\n",
    "            i = i+1\n",
    "            (running_batch, train_batch,\n",
    "                done, num_running_batch) = get_training_batch(num_running_batch,\n",
    "                                                              running_batch,\n",
    "                                                              dataset)\n",
    "            # print(i, num_running_batch, done)\n",
    "            if done:\n",
    "                scheduler.step()\n",
    "                # load sample\n",
    "                x1 = train_batch['previmg'].to(device)\n",
    "                x2 = train_batch['currimg'].to(device)\n",
    "                y = train_batch['currbb'].requires_grad_(False).to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                output = model(x1, x2)\n",
    "                loss = criterion(output, y)\n",
    "\n",
    "                # backward + optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                curr_loss = loss.item()\n",
    "                end = time.time()\n",
    "                itr = itr + 1\n",
    "                print('[training] step = %d/%d, loss = %f, time = %f'\n",
    "                      % (itr, num_batches, curr_loss, end-st))\n",
    "                sys.stdout.flush()\n",
    "                del(train_batch)\n",
    "                st = time.time()\n",
    "\n",
    "                if enable_tensorboard:\n",
    "                    writer.add_scalar('train/batch_loss', curr_loss, itr)\n",
    "\n",
    "                if itr > 0 and itr % kSaveModel == 0:\n",
    "                    path = os.path.join(save_directory,\n",
    "                                        'model_itr_' + str(itr) + '_loss_' +\n",
    "                                        str(round(curr_loss, 3)) + '.pth.tar')\n",
    "                    save_checkpoint({'itr': itr,\n",
    "                                     'np_rand_state': np.random.get_state(),\n",
    "                                     'torch_rand_state': torch.get_rng_state(),\n",
    "                                     'l1_loss': curr_loss,\n",
    "                                     'state_dict': model.state_dict(),\n",
    "                                     'optimizer': optimizer.state_dict(),\n",
    "                                     'scheduler': scheduler.state_dict(),\n",
    "                                     'num_running_batch': num_running_batch,\n",
    "                                     'running_batch': running_batch,\n",
    "                                     'lr': lr,\n",
    "                                     'dataset_indx': i}, path)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    if enable_tensorboard:\n",
    "        writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "        writer.close()\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global args, batchSize, kSaveModel, bb_params\n",
    "#args = parser.parse_args()\n",
    "#print(args)\n",
    "batchSize = 30\n",
    "kSaveModel = 20\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# load bounding box motion model params\n",
    "#bb_params['lambda_shift_frac'] = args.lambda_shift_frac\n",
    "#bb_params['lambda_scale_frac'] = args.lambda_scale_frac\n",
    "#bb_params['min_scale'] = args.min_scale\n",
    "#bb_params['max_scale'] = args.max_scale\n",
    "\n",
    "# load datasets    \n",
    "datasets = load_dataset().dataset\n",
    "\n",
    "# list of datasets to train on\n",
    "#datasets = [alov, imagenet]\n",
    "#datasets = imagenet\n",
    "\n",
    "# load model\n",
    "net = model.GoNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory ../saved_checkpoints/exp3/ already exists\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'get_orig_sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-1121ba5a88af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# save trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-96-0c4c94409a11>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, datasets, criterion, optimizer)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_running_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_running_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                                                               \u001b[0mrunning_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                                                               dataset)\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0;31m# print(i, num_running_batch, done)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-96-0c4c94409a11>\u001b[0m in \u001b[0;36mget_training_batch\u001b[0;34m(num_running_batch, running_batch, dataset)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkGeneratedExamplesPerImage\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mx1_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mx2_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mcount_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchSize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnum_running_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-96-0c4c94409a11>\u001b[0m in \u001b[0;36mmake_transformed_samples\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# unscaled original sample (single image and bb)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0morig_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_orig_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;31m# cropped scaled sample (two frames and bb)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mtrue_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'get_orig_sample'"
     ]
    }
   ],
   "source": [
    "# summary(net, [(3, 224, 224), (3, 224, 224)])\n",
    "loss_fn = torch.nn.L1Loss(size_average=False).to(device)\n",
    "\n",
    "# initialize optimizer\n",
    "optimizer = optim.SGD(net.classifier.parameters(),\n",
    "                      lr=lr,\n",
    "                   momentum=momentum,\n",
    "                     weight_decay=weight_decay)\n",
    "\n",
    "if os.path.exists(save_directory):\n",
    "    print('Directory %s already exists' % (save_directory))\n",
    "else:\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "# start training\n",
    "net = train_model(net, datasets, loss_fn, optimizer)\n",
    "\n",
    "# save trained model\n",
    "checkpoint = {'state_dict': net.state_dict()}\n",
    "path = os.path.join(save_directory, 'pytorch_goturn.pth.tar')\n",
    "torch.save(checkpoint, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(folder='bear'):\n",
    "    data_path = '../sequences-train/'\n",
    "    dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        transform=torchvision.transforms.ToTensor()\n",
    "    )\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=64,\n",
    "        num_workers=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1006"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-e4ebafe60bed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dataloader.dataset, 0):\n",
    "    plt.imshow(Image(data))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
